{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a16a32ad",
   "metadata": {},
   "source": [
    "# Text Classification Using RNN\n",
    "\n",
    "In this work the aim is to classify text phrases into 5 categories using a recurrent neural network. The categories are politics, sports, technology, entertainment and business. The dataset used can be found at: https://www.kaggle.com/datasets/tanishqdublish/text-classification-documentation.\n",
    "\n",
    "PyTorch is used to create the RNN, credits to the following tutorial, the structure of the script is based on it: https://www.geeksforgeeks.org/deep-learning/implementing-recurrent-neural-networks-in-pytorch/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b05fdc6",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0712b5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf3e96f",
   "metadata": {},
   "source": [
    "## Read and preprocess data\n",
    "\n",
    "* Lowercasing text, dividing into words\n",
    "* Building vocabulary from the words\n",
    "* Encoding text to numbers\n",
    "\n",
    "Even though the avg sequence size is almost 400, with a simple and quite small model the best results were achieved with only 50 first words. If the sequence length is smaller than 50, 0's are added to the end of its' encoded version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9111e797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sequence length: 384.04044943820224\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "df = pd.read_csv('df_file.csv')\n",
    "\n",
    "# Text preprocessing: lowercasing and tokenization\n",
    "df['Text'] = df['Text'].str.lower().str.split()\n",
    "\n",
    "# Ensure correct label encoding\n",
    "le = LabelEncoder()\n",
    "df['Label'] = le.fit_transform(df['Label'])\n",
    "\n",
    "trainData, testData = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build vocabulary\n",
    "vocab = {word for phrase in df['Text'] for word in phrase}\n",
    "wordToIdx = {word: idx for idx, word in enumerate(vocab, start=1)}  # Start indexing from 1\n",
    "\n",
    "# Calculate average sequence length\n",
    "length = 0\n",
    "for text in df['Text']:\n",
    "    length += len(text)\n",
    "avgLength = length / len(df['Text'])\n",
    "print(f\"Average sequence length: {avgLength}\")\n",
    "\n",
    "maxSeqLength = 50\n",
    "\n",
    "# Encode and pad sequences\n",
    "def encode_and_pad(phrase, wordToIdx, maxSeqLength):\n",
    "    encoded = [wordToIdx[word] for word in phrase]\n",
    "    if len(encoded) <= maxSeqLength:\n",
    "        return encoded + [0] * (maxSeqLength - len(encoded))\n",
    "    else:\n",
    "        return encoded[:maxSeqLength]\n",
    "\n",
    "trainData['Text'] = trainData['Text'].apply(lambda x: encode_and_pad(x, wordToIdx, maxSeqLength))\n",
    "testData['Text'] = testData['Text'].apply(lambda x: encode_and_pad(x, wordToIdx, maxSeqLength))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171c4eff",
   "metadata": {},
   "source": [
    "## Create dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b48d110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Dataset and DataLoader for our data\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.texts = data['Text'].values\n",
    "        self.labels = data['Label'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.texts[idx], dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "    \n",
    "trainData = TextDataset(trainData)\n",
    "testData = TextDataset(testData)\n",
    "\n",
    "batchsize = 32\n",
    "\n",
    "trainLoader = DataLoader(trainData, batch_size=batchsize, shuffle=True)\n",
    "testLoader = DataLoader(testData, batch_size=batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83422d2",
   "metadata": {},
   "source": [
    "## Creating RNN model\n",
    "\n",
    "A simple RNN model consisting of an embedding layer, 1 RNN layer, and output layer is used. The size of embeddings is 64, and we have 64 features in the hidden state. The initial hidden state consists of zeros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0ce67d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size) # store word embeddings\n",
    "        self.rnn = nn.RNN(embed_size, hidden_size, num_layers=n_layers, batch_first=True) # RNN layer, where recurrent connections happen :D\n",
    "        self.fc = nn.Linear(hidden_size, output_size) # output layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) # convert word indices to embeddings\n",
    "        h0 = torch.zeros(1, x.size(0), hidden_size).to(x.device) # initial hidden state\n",
    "        output, _ = self.rnn(x, h0) # RNN forward pass\n",
    "        output = self.fc(output[:, -1, :]) # use the last time step's output for classification\n",
    "        return output\n",
    "    \n",
    "vocab_size = len(wordToIdx) + 1  \n",
    "embed_size = 64\n",
    "hidden_size = 64\n",
    "output_size = 5\n",
    "model = RNNModel(vocab_size, embed_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097746a3",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We train the model for 50 epochs using cross-entropy loss function and Adam optimizer with the learning rate of 0.001.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "66f224be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 1.6299247933285577\n",
      "Epoch 2/50, Loss: 1.5395584042583192\n",
      "Epoch 3/50, Loss: 1.4711667725018092\n",
      "Epoch 4/50, Loss: 1.374806489263262\n",
      "Epoch 5/50, Loss: 1.2411382900817054\n",
      "Epoch 6/50, Loss: 1.0497327936547143\n",
      "Epoch 7/50, Loss: 0.8562618046998978\n",
      "Epoch 8/50, Loss: 0.6639349763946873\n",
      "Epoch 9/50, Loss: 0.5045583301356861\n",
      "Epoch 10/50, Loss: 0.34392036284719196\n",
      "Epoch 11/50, Loss: 0.2504749002733401\n",
      "Epoch 12/50, Loss: 0.17141771675752743\n",
      "Epoch 13/50, Loss: 0.15898248200703943\n",
      "Epoch 14/50, Loss: 0.09946302969806961\n",
      "Epoch 15/50, Loss: 0.06025222432799637\n",
      "Epoch 16/50, Loss: 0.04152095021813044\n",
      "Epoch 17/50, Loss: 0.031076891281242882\n",
      "Epoch 18/50, Loss: 0.024062717449851334\n",
      "Epoch 19/50, Loss: 0.020134851469525268\n",
      "Epoch 20/50, Loss: 0.016411327124972428\n",
      "Epoch 21/50, Loss: 0.013360466342419386\n",
      "Epoch 22/50, Loss: 0.011416962112499667\n",
      "Epoch 23/50, Loss: 0.009803800269894834\n",
      "Epoch 24/50, Loss: 0.008472006784618966\n",
      "Epoch 25/50, Loss: 0.007443025334006441\n",
      "Epoch 26/50, Loss: 0.0066256276539726445\n",
      "Epoch 27/50, Loss: 0.005913100525503978\n",
      "Epoch 28/50, Loss: 0.005338276008842513\n",
      "Epoch 29/50, Loss: 0.004800828173756599\n",
      "Epoch 30/50, Loss: 0.004371528297529689\n",
      "Epoch 31/50, Loss: 0.003970949419973684\n",
      "Epoch 32/50, Loss: 0.003638932327573587\n",
      "Epoch 33/50, Loss: 0.003332543673293133\n",
      "Epoch 34/50, Loss: 0.003061383852452439\n",
      "Epoch 35/50, Loss: 0.0028463453594927807\n",
      "Epoch 36/50, Loss: 0.0026305962570144664\n",
      "Epoch 37/50, Loss: 0.002504921312460543\n",
      "Epoch 38/50, Loss: 0.1161617382832836\n",
      "Epoch 39/50, Loss: 0.05951901380571404\n",
      "Epoch 40/50, Loss: 0.01622934979552935\n",
      "Epoch 41/50, Loss: 0.006384431575757584\n",
      "Epoch 42/50, Loss: 0.004346670233644545\n",
      "Epoch 43/50, Loss: 0.003309957038644435\n",
      "Epoch 44/50, Loss: 0.002860972354288346\n",
      "Epoch 45/50, Loss: 0.002522953012625554\n",
      "Epoch 46/50, Loss: 0.0022654805132853134\n",
      "Epoch 47/50, Loss: 0.0020476373881267917\n",
      "Epoch 48/50, Loss: 0.0018664646112093969\n",
      "Epoch 49/50, Loss: 0.0017252712069811033\n",
      "Epoch 50/50, Loss: 0.0015901404580550402\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for texts, labels in trainLoader:\n",
    "        outputs = model(texts)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(trainLoader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d76afd2",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "After the training we evaluate model's performance with unseen test data. The maximum value from the output is checked to define the predicted class. We were able to achieve a bit over 50% accuracy with this setup. The performance could be improved with a bigger model, more layers, and more data used. Also, LSTM model should overperform RNN in this task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "df85792b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 53.93%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, labels in testLoader:\n",
    "        outputs = model(texts)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fef4576",
   "metadata": {},
   "source": [
    "Checking the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f1085729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: tensor([4, 2, 4, 2, 0, 1, 1, 2, 1, 0, 1, 1, 4, 2, 2, 4, 2, 4, 4, 4, 2, 4, 3, 2,\n",
      "        1, 0, 4, 1, 3]), Actual: tensor([3, 2, 4, 2, 1, 2, 1, 2, 3, 0, 1, 2, 2, 2, 3, 2, 1, 4, 3, 4, 4, 4, 3, 2,\n",
      "        1, 0, 2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Predicted: {predicted}, Actual: {labels}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
